name: Solr Data Sync Across Multiple BK Servers

on:
  workflow_dispatch:

jobs:
  data-transfer-and-clean:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install jq

      - name: Get Master Solr Server URL
        id: solr-server
        run: |
          echo "MASTER_SOLR_URL=$(curl -s https://api.peviitor.ro/devops/solr/)" >> $GITHUB_ENV

      - name: Fetch Data from Master Solr
        run: |
          curl "${{ env.MASTER_SOLR_URL }}/solr/jobs/select?q=*:*&fl=job_title,job_link,company,hiringOrganization.name,country,remote,jobLocationType,validThrough,city,sursa,id,county&rows=1000000&wt=json&indent=true" -o backup.json

      - name: Clean the JSON File
        run: jq '.response.docs' backup.json > cleaned_backup.json

      # Define servers array for better management and iteration
      - name: Define Backup Servers
        run: echo "BACKUP_SERVERS=http://peviitor.go.ro:80,http://zimbor.go.ro:8985,http://84.117.157.10:8983,http://srvalx.duckdns.org:8983" >> $GITHUB_ENV

      # Iterating over each backup server and uploading data
      - name: Upload Data to Backup Servers
        run: |
          for server in ${BACKUP_SERVERS//,/ }
          do
            echo "Cleaning data on $server"
            curl "$server/solr/jobs/update" -H "Content-Type: text/xml" --data-binary '<delete><query>*:*</query></delete>'
            curl "$server/solr/jobs/update" --data '<commit/>'
            echo "Uploading data to $server"
            curl "$server/solr/jobs/update?commit=true" -H "Content-Type: application/json" --data-binary @cleaned_backup.json
          done
